{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/priyanshu1303d/Projects/DeepQA_PyTorch/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/priyanshu1303d/Projects/DeepQA_PyTorch'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir:Path\n",
    "    saved_model_path: Path\n",
    "    model_metrics_json: Path\n",
    "    vocab_file_path : Path\n",
    "    data_path : Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from DeepQA.constants import *\n",
    "from DeepQA.utils.common import read_yaml , create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(self , config_filepath = CONFIG_FILE_PATH , params_filepath = PARAMS_FILE_PATH):\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        config = self.config.model_evaluation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        model_evaluation_config = ModelEvaluationConfig(\n",
    "            root_dir= config.root_dir,\n",
    "            model_metrics_json= config.model_metrics_json,\n",
    "            saved_model_path= config.saved_model_path,\n",
    "            vocab_file_path= config.vocab_file_path,\n",
    "            data_path= config.data_path\n",
    "\n",
    "        )\n",
    "\n",
    "        return model_evaluation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA_Dataclass(Dataset):\n",
    "    def __init__(self , df , vocab):\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        print(type(self.df))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Convert string representations of lists back to actual lists\n",
    "        numerical_question = ast.literal_eval(self.df.iloc[index]['question_indices'])\n",
    "        numerical_answer = ast.literal_eval(self.df.iloc[index]['answer_indices'])\n",
    "\n",
    "        # Convert to PyTorch tensors\n",
    "        question_tensor = torch.tensor(numerical_question, dtype=torch.long).unsqueeze(0)  # Add batch dimension\n",
    "        answer_tensor = torch.tensor(numerical_answer, dtype=torch.long)\n",
    "\n",
    "        return question_tensor, answer_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.serialization\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=50, hidden_size=64):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x) \n",
    "\n",
    "        # ðŸš¨ Fix: Remove the extra dimension if needed\n",
    "        if x.dim() == 4:  \n",
    "            x = x.squeeze(1)  # Remove the unnecessary 1-dim (batch_size, 1, seq_len, embedding_dim) â†’ (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        output, hidden = self.rnn(x)  # Pass through RNN\n",
    "        output = self.fc(output[:, -1, :])  # Take the last output for classification\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "        with open(config.vocab_file_path, 'r') as f:\n",
    "            vocab = json.load(f)\n",
    "        self.vocab_size = len(vocab)\n",
    "\n",
    "        # Define the model architecture (same as training)\n",
    "        self.model = RNNModel(vocab_size=self.vocab_size).to(self.device)\n",
    "\n",
    "        # Load model weights\n",
    "        self.model = RNNModel(vocab_size=self.vocab_size).to(self.device)  # Define model\n",
    "        self.model.load_state_dict(torch.load(self.config.saved_model_path, map_location=self.device))  # âœ… Load weights\n",
    "        self.model.eval()\n",
    "\n",
    "        df = pd.read_csv(self.config.data_path)  # âœ… Ensure it's a DataFrame\n",
    "\n",
    "        self.dataset = QA_Dataclass(df, vocab)\n",
    "        self.test_loader = DataLoader(self.dataset, batch_size=1, shuffle=True, pin_memory=True)\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in self.test_loader:\n",
    "                inputs, labels = batch  \n",
    "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
    "\n",
    "                outputs = self.model(inputs)\n",
    "                predictions = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                all_preds.extend(predictions.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "        print(f\"ðŸ”¹ Accuracy: {accuracy * 100:.2f}%\")\n",
    "        print(f\"ðŸ”¹ F1 Score: {f1:.4f}\")\n",
    "\n",
    "        return accuracy, f1\n",
    "    \n",
    "    def save_metrics(self, accuracy, f1):\n",
    "        \"\"\"Save accuracy and F1-score to a JSON file.\"\"\"\n",
    "        results = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"f1_score\": f1\n",
    "        }\n",
    "\n",
    "        # Ensure the directory exists\n",
    "        model_mertics_path = Path(self.config.model_metrics_json)\n",
    "        os.makedirs(model_mertics_path, exist_ok=True)\n",
    "\n",
    "        # Save results to JSON\n",
    "        results_file = os.path.join(model_mertics_path, \"results.json\")\n",
    "        with open(results_file, \"w\") as f:\n",
    "            json.dump(results, f, indent=4)\n",
    "\n",
    "        print(f\"âœ… Evaluation metrics saved at: {results_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-03 19:20:06,283 : INFO : common  : yaml file config/config.yaml was read succesfully]\n",
      "[2025-04-03 19:20:06,286 : INFO : common  : yaml file params.yaml was read succesfully]\n",
      "[2025-04-03 19:20:06,286 : INFO : common  : Created directory at : artifacts]\n",
      "[2025-04-03 19:20:06,287 : INFO : common  : Created directory at : artifacts/model_evaluation]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "ðŸ”¹ Accuracy: 100.00%\n",
      "ðŸ”¹ F1 Score: 1.0000\n",
      "1.0   1.0\n",
      "âœ… Evaluation metrics saved at: artifacts/model_evaluation/results.json\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    get_model_evaluation_config = config.get_model_evaluation_config()\n",
    "    model_evaluation = ModelEvaluation(get_model_evaluation_config)\n",
    "    \n",
    "    accuracy , f1 = model_evaluation.evaluate_model()\n",
    "    print(f1 ,\" \" ,accuracy)\n",
    "    model_evaluation.save_metrics(accuracy, f1)\n",
    "\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepQA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
